# 人工标注核验平台

## 功能特点

### 🎯 核心功能
- **数据加载**: 自动读取 `../data_generator/data/` 目录下的JSON数据文件
- **情境展示**: 清晰展示场景描述和角色设定
- **对话阅读**: 高亮显示关键时刻的对话内容
- **标注界面**: 三个评测任务的选择题界面
- **答案对比**: 实时对比人工答案与原始生成答案
- **数据保存**: 人工标注结果保存到 `annotated_data/` 目录

### 📊 评测任务
1. **氛围识别**: 判断对话的整体氛围和情感基调
2. **KY测试**: 评估角色的情商和社交敏感度
3. **意图推断**: 分析角色的真实意图和动机

### 🔄 工作流程
1. 选择数据文件并加载
2. 阅读情境设定和角色信息
3. 浏览完整对话内容
4. 对三个评测任务进行标注
5. 查看与原始答案的对比
6. 保存人工标注结果

## 安装和运行

### 1. 安装依赖
```bash
cd /home/Group/platform
pip install -r requirements.txt
```

### 2. 启动服务
```bash
python app.py
```

### 3. 访问平台
打开浏览器访问: http://localhost:5000

## 目录结构
```
platform/
├── app.py                 # Flask后端应用
├── requirements.txt       # Python依赖
├── README.md             # 说明文档
├── templates/
│   └── index.html        # 主页模板
├── static/
│   ├── css/
│   │   └── style.css     # 样式文件
│   └── js/
│       └── app.js        # 前端逻辑
└── annotated_data/       # 标注结果保存目录
```

## 数据格式

### 输入数据
- 位置: `../data_generator/data/*.json`
- 格式: 标准JSON格式的数据集文件

### 输出数据
- 位置: `annotated_data/annotated_*.json`
- 格式: 包含人工标注结果的完整数据集
- 特点: 保留原始答案，添加人工标注标记

## 使用说明

### 界面布局
- **左侧**: 情境设定和对话内容
- **右侧**: 评测标签和标注界面
- **顶部**: 文件选择和数据集信息

### 标注操作
1. **选择答案**: 点击选项进行选择
2. **查看对比**: 实时显示与原始答案的差异
3. **保存标注**: 点击保存按钮确认标注
4. **导航切换**: 使用上一个/下一个按钮或跳转功能

### 视觉提示
- 🟢 **绿色边框**: 原始生成的答案
- 🔵 **蓝色背景**: 当前选中的答案  
- 🟡 **黄色背景**: 与原始答案不同的选择
- 🔴 **红色边框**: 关键时刻的对话

## 技术栈
- **后端**: Flask + Python
- **前端**: Bootstrap 5 + Vanilla JavaScript
- **样式**: CSS3 + Font Awesome图标
- **数据**: JSON格式存储
