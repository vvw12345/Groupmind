"""
AIæ ‡æ³¨å‡†ç¡®ç‡å¯è§†åŒ–åˆ†æ
ç”Ÿæˆç±»ä¼¼è®ºæ–‡ä¸­çš„è¡¨æ ¼å’Œå›¾è¡¨
"""
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from pathlib import Path

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class VisualizationGenerator:
    """å¯è§†åŒ–ç”Ÿæˆå™¨"""
    
    def __init__(self, report_file: str = "ai_human_comparison_report.json"):
        self.report_file = report_file
        self.results = None
        self.load_results()
    
    def load_results(self):
        """åŠ è½½åˆ†æç»“æœ"""
        try:
            with open(self.report_file, 'r', encoding='utf-8') as f:
                self.results = json.load(f)
            print(f"âœ… åŠ è½½åˆ†æç»“æœ: {self.report_file}")
        except FileNotFoundError:
            print(f"âŒ åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {self.report_file}")
            print("è¯·å…ˆè¿è¡Œ python analysis.py ç”Ÿæˆåˆ†ææŠ¥å‘Š")
    
    def create_accuracy_table(self):
        """åˆ›å»ºå‡†ç¡®ç‡è¡¨æ ¼ (ç±»ä¼¼è®ºæ–‡Table 2)"""
        if not self.results:
            return
        
        # å‡†å¤‡æ•°æ®
        data = []
        
        # æ•´ä½“å‡†ç¡®ç‡
        overall_acc = self.results['overall_accuracy'] * 100
        data.append(['Overall', f"{overall_acc:.1f}%", 
                    f"{self.results['annotated_samples']}", "DeepSeek V3"])
        
        # å„ä»»åŠ¡å‡†ç¡®ç‡
        for task_key, task_data in self.results['task_accuracy'].items():
            acc = task_data['accuracy'] * 100
            data.append([
                task_data['task_name'],
                f"{acc:.1f}%",
                f"{task_data['correct_count']}/{task_data['total_count']}",
                self._get_confidence_level(acc)
            ])
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(data, columns=['Task', 'Accuracy', 'Correct/Total', 'Confidence'])
        
        # åˆ›å»ºè¡¨æ ¼å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.axis('tight')
        ax.axis('off')
        
        table = ax.table(cellText=df.values,
                        colLabels=df.columns,
                        cellLoc='center',
                        loc='center')
        
        table.auto_set_font_size(False)
        table.set_fontsize(12)
        table.scale(1.2, 1.5)
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for i in range(len(df.columns)):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        plt.title('DeepSeek V3 vs Human Annotation Accuracy', 
                 fontsize=16, fontweight='bold', pad=20)
        plt.savefig('accuracy_table.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return df
    
    def create_task_comparison_chart(self):
        """åˆ›å»ºä»»åŠ¡å¯¹æ¯”å›¾è¡¨"""
        if not self.results:
            return
        
        # å‡†å¤‡æ•°æ®
        tasks = []
        accuracies = []
        difficulties = []
        
        for task_key, task_data in self.results['task_accuracy'].items():
            tasks.append(task_data['task_name'])
            accuracies.append(task_data['accuracy'] * 100)
            
            # è·å–éš¾åº¦ä¿¡æ¯
            conf_data = self.results['confidence_analysis']['task_confidence'].get(task_key, {})
            difficulties.append(conf_data.get('difficulty_level', 'æœªçŸ¥'))
        
        # åˆ›å»ºæŸ±çŠ¶å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å‡†ç¡®ç‡æŸ±çŠ¶å›¾
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        bars = ax1.bar(tasks, accuracies, color=colors)
        ax1.set_ylabel('Accuracy (%)')
        ax1.set_title('Task-wise Accuracy Comparison')
        ax1.set_ylim(0, 100)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{acc:.1f}%', ha='center', va='bottom')
        
        # éš¾åº¦åˆ†å¸ƒé¥¼å›¾
        difficulty_counts = pd.Series(difficulties).value_counts()
        ax2.pie(difficulty_counts.values, labels=difficulty_counts.index, 
               autopct='%1.1f%%', startangle=90)
        ax2.set_title('Task Difficulty Distribution')
        
        plt.tight_layout()
        plt.savefig('task_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_theme_analysis_chart(self):
        """åˆ›å»ºä¸»é¢˜åˆ†æå›¾è¡¨"""
        if not self.results:
            return
        
        theme_data = self.results['confidence_analysis']['theme_confidence']
        
        if not theme_data:
            print("âŒ æ²¡æœ‰ä¸»é¢˜æ•°æ®å¯ä¾›åˆ†æ")
            return
        
        # å‡†å¤‡æ•°æ®
        themes = list(theme_data.keys())
        accuracies = [data['mean_accuracy'] * 100 for data in theme_data.values()]
        std_devs = [data['std_accuracy'] * 100 for data in theme_data.values()]
        sample_counts = [data['sample_count'] for data in theme_data.values()]
        
        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # ä¸»é¢˜å‡†ç¡®ç‡æ¡å½¢å›¾ï¼ˆå¸¦è¯¯å·®æ¡ï¼‰
        y_pos = np.arange(len(themes))
        bars = ax1.barh(y_pos, accuracies, xerr=std_devs, 
                       color='skyblue', alpha=0.7, capsize=5)
        ax1.set_yticks(y_pos)
        ax1.set_yticklabels(themes)
        ax1.set_xlabel('Accuracy (%)')
        ax1.set_title('Theme-wise Accuracy with Confidence Intervals')
        ax1.set_xlim(0, 100)
        
        # æ·»åŠ æ ·æœ¬æ•°é‡æ ‡ç­¾
        for i, (bar, count) in enumerate(zip(bars, sample_counts)):
            width = bar.get_width()
            ax1.text(width + 2, bar.get_y() + bar.get_height()/2,
                    f'n={count}', ha='left', va='center')
        
        # æ ·æœ¬æ•°é‡åˆ†å¸ƒ
        ax2.bar(themes, sample_counts, color='lightcoral', alpha=0.7)
        ax2.set_ylabel('Sample Count')
        ax2.set_title('Sample Distribution by Theme')
        ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('theme_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_agreement_heatmap(self):
        """åˆ›å»ºä¸€è‡´æ€§çƒ­åŠ›å›¾"""
        if not self.results:
            return
        
        agreement_data = self.results['agreement_matrix']
        
        if not agreement_data:
            print("âŒ æ²¡æœ‰ä¸€è‡´æ€§æ•°æ®å¯ä¾›åˆ†æ")
            return
        
        # å‡†å¤‡æ•°æ®
        tasks = []
        agreements = []
        
        for task_key, data in agreement_data.items():
            task_name = self.results['task_accuracy'][task_key]['task_name']
            tasks.append(task_name)
            agreements.append(data['agreement_rate'])
        
        # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®çŸ©é˜µ
        agreement_matrix = np.array(agreements).reshape(1, -1)
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(10, 3))
        sns.heatmap(agreement_matrix, 
                   xticklabels=tasks,
                   yticklabels=['AI vs Human'],
                   annot=True, 
                   fmt='.1f',
                   cmap='RdYlGn',
                   vmin=0, vmax=100,
                   cbar_kws={'label': 'Agreement Rate (%)'})
        
        plt.title('Inter-Annotator Agreement (AI vs Human)')
        plt.tight_layout()
        plt.savefig('agreement_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def _get_confidence_level(self, accuracy: float) -> str:
        """æ ¹æ®å‡†ç¡®ç‡è·å–ç½®ä¿¡åº¦ç­‰çº§"""
        if accuracy >= 95:
            return "Very High"
        elif accuracy >= 85:
            return "High"
        elif accuracy >= 70:
            return "Medium"
        elif accuracy >= 50:
            return "Low"
        else:
            return "Very Low"
    
    def generate_all_visualizations(self):
        """ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
        if not self.results:
            print("âŒ æ— æ³•ç”Ÿæˆå¯è§†åŒ–ï¼Œè¯·å…ˆè¿è¡Œåˆ†æ")
            return
        
        print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        try:
            self.create_accuracy_table()
            print("âœ… ç”Ÿæˆå‡†ç¡®ç‡è¡¨æ ¼")
            
            self.create_task_comparison_chart()
            print("âœ… ç”Ÿæˆä»»åŠ¡å¯¹æ¯”å›¾è¡¨")
            
            self.create_theme_analysis_chart()
            print("âœ… ç”Ÿæˆä¸»é¢˜åˆ†æå›¾è¡¨")
            
            self.create_agreement_heatmap()
            print("âœ… ç”Ÿæˆä¸€è‡´æ€§çƒ­åŠ›å›¾")
            
            print("\nğŸ‰ æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆï¼")
            print("ğŸ“ å›¾ç‰‡æ–‡ä»¶:")
            print("  - accuracy_table.png")
            print("  - task_comparison.png") 
            print("  - theme_analysis.png")
            print("  - agreement_heatmap.png")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¯è§†åŒ–æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    viz = VisualizationGenerator()
    viz.generate_all_visualizations()

if __name__ == "__main__":
    main()
