"""
æ ‡æ³¨åˆ†ææ¨¡å— - è®¡ç®—IAAç³»æ•°å’Œæ¨¡å‹å‡†ç¡®ç‡
ç”¨äºè®ºæ–‡ä¸­è¯æ˜å¤§æ¨¡å‹æ ‡ç­¾çš„æœ‰æ•ˆæ€§
"""

import json
import numpy as np
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any
import pandas as pd

class AnnotationAnalyzer:
    """æ ‡æ³¨åˆ†æå™¨"""
    
    def __init__(self, annotated_file_path: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            annotated_file_path: æ ‡æ³¨åçš„æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.file_path = Path(annotated_file_path)
        self.data = self._load_data()
        self.annotated_samples = self._get_annotated_samples()
        
    def _load_data(self) -> Dict:
        """åŠ è½½æ ‡æ³¨æ•°æ®"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _get_annotated_samples(self) -> List[Dict]:
        """è·å–å·²æ ‡æ³¨çš„æ ·æœ¬"""
        return [sample for sample in self.data['samples'] 
                if sample.get('human_annotated', False)]
    
    def calculate_agreement_metrics(self) -> Dict[str, Any]:
        """
        è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
        
        Returns:
            åŒ…å«å„ç§ä¸€è‡´æ€§æŒ‡æ ‡çš„å­—å…¸
        """
        if not self.annotated_samples:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°å·²æ ‡æ³¨çš„æ ·æœ¬"}
        
        results = {
            "total_annotated_samples": len(self.annotated_samples),
            "total_samples": len(self.data['samples']),
            "annotation_coverage": len(self.annotated_samples) / len(self.data['samples']) * 100,
            "task_metrics": {}
        }
        
        # åˆ†ä»»åŠ¡è®¡ç®—æŒ‡æ ‡
        tasks = ['atmosphere_recognition', 'ky_test', 'subtext_deciphering']
        task_names = {
            'atmosphere_recognition': 'æ°›å›´è¯†åˆ«',
            'ky_test': 'KYæµ‹è¯•', 
            'subtext_deciphering': 'æ½œå°è¯è§£ç '
        }
        
        for task in tasks:
            task_result = self._calculate_task_agreement(task)
            results["task_metrics"][task_names[task]] = task_result
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        results["overall_metrics"] = self._calculate_overall_metrics()
        
        return results
    
    def _calculate_task_agreement(self, task: str) -> Dict[str, float]:
        """è®¡ç®—å•ä¸ªä»»åŠ¡çš„ä¸€è‡´æ€§æŒ‡æ ‡"""
        original_answers = []
        human_answers = []
        
        for sample in self.annotated_samples:
            if task in sample.get('original_labels', {}):
                orig_idx = sample['original_labels'][task]['correct_answer_index']
                human_idx = sample['evaluation_labels'][task]['correct_answer_index']
                
                original_answers.append(orig_idx)
                human_answers.append(human_idx)
        
        if not original_answers:
            return {"error": f"ä»»åŠ¡ {task} æ²¡æœ‰æœ‰æ•ˆæ•°æ®"}
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆä¸€è‡´æ€§ï¼‰
        agreements = [1 if orig == human else 0 
                     for orig, human in zip(original_answers, human_answers)]
        accuracy = np.mean(agreements) * 100
        
        # è®¡ç®—Kappaç³»æ•°
        kappa = self._calculate_kappa(original_answers, human_answers)
        
        # è®¡ç®—æ··æ·†çŸ©é˜µç»Ÿè®¡
        confusion_stats = self._calculate_confusion_stats(original_answers, human_answers)
        
        return {
            "sample_count": len(original_answers),
            "accuracy": round(accuracy, 2),
            "agreement_rate": round(accuracy, 2),  # åŒaccuracyï¼Œä½†è¯­ä¹‰æ›´æ¸…æ™°
            "kappa_coefficient": round(kappa, 3),
            "kappa_interpretation": self._interpret_kappa(kappa),
            "confusion_stats": confusion_stats,
            "disagreement_cases": len(original_answers) - sum(agreements),
            "disagreement_rate": round((1 - np.mean(agreements)) * 100, 2)
        }
    
    def _calculate_kappa(self, list1: List[int], list2: List[int]) -> float:
        """è®¡ç®—Cohen's Kappaç³»æ•°"""
        if len(list1) != len(list2):
            return 0.0
        
        n = len(list1)
        if n == 0:
            return 0.0
        
        # è·å–æ‰€æœ‰å¯èƒ½çš„ç±»åˆ«
        all_categories = sorted(set(list1 + list2))
        k = len(all_categories)
        
        if k <= 1:
            return 1.0  # å®Œå…¨ä¸€è‡´
        
        # åˆ›å»ºæ··æ·†çŸ©é˜µ
        confusion_matrix = np.zeros((k, k))
        cat_to_idx = {cat: i for i, cat in enumerate(all_categories)}
        
        for a1, a2 in zip(list1, list2):
            i, j = cat_to_idx[a1], cat_to_idx[a2]
            confusion_matrix[i][j] += 1
        
        # è®¡ç®—è§‚å¯Ÿåˆ°çš„ä¸€è‡´æ€§
        po = np.trace(confusion_matrix) / n
        
        # è®¡ç®—æœŸæœ›ä¸€è‡´æ€§
        marginal1 = np.sum(confusion_matrix, axis=1) / n
        marginal2 = np.sum(confusion_matrix, axis=0) / n
        pe = np.sum(marginal1 * marginal2)
        
        # è®¡ç®—Kappa
        if pe == 1.0:
            return 1.0
        
        kappa = (po - pe) / (1 - pe)
        return kappa
    
    def _interpret_kappa(self, kappa: float) -> str:
        """è§£é‡ŠKappaç³»æ•°"""
        if kappa < 0:
            return "å·®äºéšæœº (Poor)"
        elif kappa < 0.20:
            return "è½»å¾®ä¸€è‡´ (Slight)"
        elif kappa < 0.40:
            return "ä¸€èˆ¬ä¸€è‡´ (Fair)"
        elif kappa < 0.60:
            return "ä¸­ç­‰ä¸€è‡´ (Moderate)"
        elif kappa < 0.80:
            return "é«˜åº¦ä¸€è‡´ (Substantial)"
        else:
            return "å‡ ä¹å®Œå…¨ä¸€è‡´ (Almost Perfect)"
    
    def _calculate_confusion_stats(self, original: List[int], human: List[int]) -> Dict:
        """è®¡ç®—æ··æ·†çŸ©é˜µç»Ÿè®¡ä¿¡æ¯"""
        # ç»Ÿè®¡æ¯ä¸ªé€‰é¡¹çš„åˆ†å¸ƒ
        original_dist = Counter(original)
        human_dist = Counter(human)
        
        # æ‰¾å‡ºåˆ†æ­§æœ€å¤§çš„é€‰é¡¹
        disagreements = defaultdict(int)
        for orig, human in zip(original, human):
            if orig != human:
                disagreements[f"{orig}â†’{human}"] += 1
        
        return {
            "original_distribution": dict(original_dist),
            "human_distribution": dict(human_dist),
            "top_disagreements": dict(sorted(disagreements.items(), 
                                           key=lambda x: x[1], reverse=True)[:5])
        }
    
    def _calculate_overall_metrics(self) -> Dict[str, float]:
        """è®¡ç®—æ€»ä½“æŒ‡æ ‡"""
        all_agreements = []
        
        tasks = ['atmosphere_recognition', 'ky_test', 'subtext_deciphering']
        
        for sample in self.annotated_samples:
            sample_agreements = []
            for task in tasks:
                if (task in sample.get('original_labels', {}) and 
                    task in sample.get('evaluation_labels', {})):
                    orig_idx = sample['original_labels'][task]['correct_answer_index']
                    human_idx = sample['evaluation_labels'][task]['correct_answer_index']
                    sample_agreements.append(1 if orig_idx == human_idx else 0)
            
            if sample_agreements:
                all_agreements.extend(sample_agreements)
        
        if not all_agreements:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„å¯¹æ¯”æ•°æ®"}
        
        overall_accuracy = np.mean(all_agreements) * 100
        
        return {
            "overall_accuracy": round(overall_accuracy, 2),
            "total_comparisons": len(all_agreements),
            "total_agreements": sum(all_agreements),
            "total_disagreements": len(all_agreements) - sum(all_agreements)
        }
    
    def generate_detailed_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        metrics = self.calculate_agreement_metrics()
        
        if "error" in metrics:
            return f"é”™è¯¯: {metrics['error']}"
        
        report = []
        report.append("=" * 60)
        report.append("æ ‡æ³¨ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        report.append("")
        
        # åŸºæœ¬ä¿¡æ¯
        report.append("ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        report.append(f"  â€¢ æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
        report.append(f"  â€¢ å·²æ ‡æ³¨æ ·æœ¬æ•°: {metrics['total_annotated_samples']}")
        report.append(f"  â€¢ æ ‡æ³¨è¦†ç›–ç‡: {metrics['annotation_coverage']:.1f}%")
        report.append("")
        
        # æ€»ä½“æŒ‡æ ‡
        if "error" not in metrics["overall_metrics"]:
            overall = metrics["overall_metrics"]
            report.append("ğŸ¯ æ€»ä½“ä¸€è‡´æ€§:")
            report.append(f"  â€¢ æ€»ä½“å‡†ç¡®ç‡: {overall['overall_accuracy']:.2f}%")
            report.append(f"  â€¢ æ€»å¯¹æ¯”æ¬¡æ•°: {overall['total_comparisons']}")
            report.append(f"  â€¢ ä¸€è‡´æ¬¡æ•°: {overall['total_agreements']}")
            report.append(f"  â€¢ åˆ†æ­§æ¬¡æ•°: {overall['total_disagreements']}")
            report.append("")
        
        # åˆ†ä»»åŠ¡æŒ‡æ ‡
        report.append("ğŸ“‹ åˆ†ä»»åŠ¡åˆ†æ:")
        for task_name, task_metrics in metrics["task_metrics"].items():
            if "error" not in task_metrics:
                report.append(f"\n  {task_name}:")
                report.append(f"    â€¢ æ ·æœ¬æ•°: {task_metrics['sample_count']}")
                report.append(f"    â€¢ ä¸€è‡´ç‡: {task_metrics['accuracy']:.2f}%")
                report.append(f"    â€¢ Kappaç³»æ•°: {task_metrics['kappa_coefficient']:.3f} ({task_metrics['kappa_interpretation']})")
                report.append(f"    â€¢ åˆ†æ­§æ¡ˆä¾‹: {task_metrics['disagreement_cases']} ({task_metrics['disagreement_rate']:.2f}%)")
                
                if task_metrics['confusion_stats']['top_disagreements']:
                    report.append(f"    â€¢ ä¸»è¦åˆ†æ­§ç±»å‹:")
                    for disagreement, count in task_metrics['confusion_stats']['top_disagreements'].items():
                        report.append(f"      - {disagreement}: {count}æ¬¡")
        
        report.append("")
        report.append("=" * 60)
        report.append("ğŸ“ è®ºæ–‡å†™ä½œå»ºè®®:")
        report.append("")
        
        # ç”Ÿæˆè®ºæ–‡å†™ä½œå»ºè®®
        overall_acc = metrics["overall_metrics"].get("overall_accuracy", 0)
        if overall_acc >= 80:
            report.append("âœ… æ¨¡å‹æ ‡ç­¾è´¨é‡è¯„ä¼°: ä¼˜ç§€")
            report.append("   å»ºè®®è¡¨è¿°: 'å¤§æ¨¡å‹ç”Ÿæˆçš„æ ‡ç­¾ä¸äººå·¥æ ‡æ³¨å…·æœ‰é«˜åº¦ä¸€è‡´æ€§'")
        elif overall_acc >= 70:
            report.append("âœ… æ¨¡å‹æ ‡ç­¾è´¨é‡è¯„ä¼°: è‰¯å¥½") 
            report.append("   å»ºè®®è¡¨è¿°: 'å¤§æ¨¡å‹ç”Ÿæˆçš„æ ‡ç­¾ä¸äººå·¥æ ‡æ³¨å…·æœ‰è¾ƒå¥½ä¸€è‡´æ€§'")
        elif overall_acc >= 60:
            report.append("âš ï¸ æ¨¡å‹æ ‡ç­¾è´¨é‡è¯„ä¼°: ä¸­ç­‰")
            report.append("   å»ºè®®è¡¨è¿°: 'å¤§æ¨¡å‹ç”Ÿæˆçš„æ ‡ç­¾ä¸äººå·¥æ ‡æ³¨å…·æœ‰ä¸­ç­‰ç¨‹åº¦ä¸€è‡´æ€§'")
        else:
            report.append("âŒ æ¨¡å‹æ ‡ç­¾è´¨é‡è¯„ä¼°: éœ€è¦æ”¹è¿›")
            report.append("   å»ºè®®è¡¨è¿°: 'å¤§æ¨¡å‹ç”Ÿæˆçš„æ ‡ç­¾éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–'")
        
        report.append("")
        report.append("ğŸ“Š å¯ç”¨äºè®ºæ–‡çš„æ•°æ®:")
        report.append(f"   â€¢ æ ‡æ³¨è€…é—´ä¸€è‡´æ€§(IAA): {overall_acc:.2f}%")
        report.append(f"   â€¢ æ ·æœ¬è¦†ç›–ç‡: {metrics['annotation_coverage']:.1f}%")
        
        # Kappaç³»æ•°æ±‡æ€»
        kappa_values = []
        for task_metrics in metrics["task_metrics"].values():
            if "kappa_coefficient" in task_metrics:
                kappa_values.append(task_metrics["kappa_coefficient"])
        
        if kappa_values:
            avg_kappa = np.mean(kappa_values)
            report.append(f"   â€¢ å¹³å‡Kappaç³»æ•°: {avg_kappa:.3f}")
        
        return "\n".join(report)
    
    def export_to_csv(self, output_path: str = None) -> str:
        """å¯¼å‡ºè¯¦ç»†æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not output_path:
            output_path = self.file_path.parent / f"annotation_analysis_{self.file_path.stem}.csv"
        
        # å‡†å¤‡æ•°æ®
        rows = []
        for sample in self.annotated_samples:
            base_info = {
                'sample_id': sample['benchmark_id'],
                'scene_index': sample.get('scene_index', ''),
                'atmosphere': sample.get('atmosphere', ''),
                'category': sample.get('scenario_setup', {}).get('category', '')
            }
            
            tasks = ['atmosphere_recognition', 'ky_test', 'subtext_deciphering']
            task_names = ['æ°›å›´è¯†åˆ«', 'KYæµ‹è¯•', 'æ½œå°è¯è§£ç ']
            
            for task, task_name in zip(tasks, task_names):
                if (task in sample.get('original_labels', {}) and 
                    task in sample.get('evaluation_labels', {})):
                    
                    orig_idx = sample['original_labels'][task]['correct_answer_index']
                    human_idx = sample['evaluation_labels'][task]['correct_answer_index']
                    
                    row = base_info.copy()
                    row.update({
                        'task': task_name,
                        'original_answer': orig_idx,
                        'human_answer': human_idx,
                        'agreement': 1 if orig_idx == human_idx else 0,
                        'original_question': sample['original_labels'][task].get('question', ''),
                        'human_question': sample['evaluation_labels'][task].get('question', '')
                    })
                    rows.append(row)
        
        # ä¿å­˜åˆ°CSV
        df = pd.DataFrame(rows)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        return str(output_path)

def analyze_annotation_file(file_path: str) -> str:
    """
    åˆ†ææ ‡æ³¨æ–‡ä»¶çš„ä¾¿æ·å‡½æ•°
    
    Args:
        file_path: æ ‡æ³¨æ–‡ä»¶è·¯å¾„
        
    Returns:
        åˆ†ææŠ¥å‘Šå­—ç¬¦ä¸²
    """
    analyzer = AnnotationAnalyzer(file_path)
    return analyzer.generate_detailed_report()

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    import sys
    
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
        print(analyze_annotation_file(file_path))
    else:
        print("ç”¨æ³•: python annotation_analysis.py <æ ‡æ³¨æ–‡ä»¶è·¯å¾„>")
