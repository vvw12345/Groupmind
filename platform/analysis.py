"""
AIä¸äººå·¥æ ‡æ³¨ç½®ä¿¡åº¦å¯¹æ¯”åˆ†æå·¥å…·
åŸºäºè®ºæ–‡ä¸­çš„åéªŒéªŒè¯æ–¹æ³•ï¼Œåˆ†æDeepSeek V3çš„æ ‡æ³¨å‡†ç¡®ç‡
"""
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple
import pandas as pd
import numpy as np
from collections import defaultdict

class AnnotationAnalyzer:
    """æ ‡æ³¨åˆ†æå™¨"""
    
    def __init__(self, annotated_dir: str = "annotated_data"):
        self.annotated_dir = Path(annotated_dir)
        self.results = {
            'total_samples': 0,
            'annotated_samples': 0,
            'task_accuracy': {},
            'overall_accuracy': 0.0,
            'agreement_matrix': {},
            'confidence_analysis': {},
            'detailed_results': []
        }
    
    def load_annotated_data(self) -> List[Dict]:
        """åŠ è½½æ‰€æœ‰æ ‡æ³¨æ•°æ®"""
        annotated_files = []
        
        if not self.annotated_dir.exists():
            print(f"âŒ æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨: {self.annotated_dir}")
            return []
        
        for file_path in self.annotated_dir.glob("annotated_*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    annotated_files.append({
                        'filename': file_path.name,
                        'data': data
                    })
                print(f"âœ… åŠ è½½æ–‡ä»¶: {file_path.name}")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {file_path.name}: {e}")
        
        return annotated_files
    
    def analyze_sample_accuracy(self, sample: Dict) -> Dict:
        """åˆ†æå•ä¸ªæ ·æœ¬çš„å‡†ç¡®ç‡"""
        if 'original_labels' not in sample or not sample.get('human_annotated', False):
            return None
        
        original = sample['original_labels']
        human = sample['evaluation_labels']
        
        task_results = {}
        
        # åˆ†æä¸‰ä¸ªä»»åŠ¡
        tasks = {
            'atmosphere_recognition': 'æ°›å›´è¯†åˆ«',
            'ky_test': 'KYæµ‹è¯•', 
            'collective_intent_inference': 'æ„å›¾æ¨æ–­'
        }
        
        for task_key, task_name in tasks.items():
            if task_key in original and task_key in human:
                ai_answer = original[task_key].get('correct_answer_index', -1)
                human_answer = human[task_key].get('correct_answer_index', -1)
                
                is_correct = ai_answer == human_answer
                task_results[task_key] = {
                    'task_name': task_name,
                    'ai_answer': ai_answer + 1,  # è½¬æ¢ä¸º1-basedæ˜¾ç¤º
                    'human_answer': human_answer + 1,
                    'is_correct': is_correct,
                    'question': human[task_key].get('question', ''),
                    'options': human[task_key].get('mcq_options', [])
                }
        
        return {
            'benchmark_id': sample.get('benchmark_id', ''),
            'meta_theme': sample.get('meta_theme', ''),
            'task_results': task_results,
            'overall_correct': sum(r['is_correct'] for r in task_results.values()),
            'total_tasks': len(task_results)
        }
    
    def calculate_inter_annotator_agreement(self, results: List[Dict]) -> Dict:
        """è®¡ç®—æ ‡æ³¨è€…é—´ä¸€è‡´æ€§(æ¨¡æ‹Ÿå¤šäººæ ‡æ³¨çš„IAA)"""
        task_agreements = {}
        
        for task_key in ['atmosphere_recognition', 'ky_test', 'collective_intent_inference']:
            agreements = []
            total_comparisons = 0
            
            for result in results:
                if task_key in result['task_results']:
                    task_result = result['task_results'][task_key]
                    # è¿™é‡Œæˆ‘ä»¬è®¡ç®—AIä¸äººå·¥çš„ä¸€è‡´æ€§ä½œä¸ºåŸºå‡†
                    agreements.append(1 if task_result['is_correct'] else 0)
                    total_comparisons += 1
            
            if total_comparisons > 0:
                agreement_rate = sum(agreements) / total_comparisons * 100
                task_agreements[task_key] = {
                    'agreement_rate': agreement_rate,
                    'total_comparisons': total_comparisons,
                    'agreements': sum(agreements)
                }
        
        return task_agreements
    
    def generate_confidence_analysis(self, results: List[Dict]) -> Dict:
        """ç”Ÿæˆç½®ä¿¡åº¦åˆ†æ"""
        theme_accuracy = defaultdict(list)
        task_difficulty = defaultdict(list)
        
        for result in results:
            theme = result['meta_theme']
            accuracy = result['overall_correct'] / result['total_tasks'] if result['total_tasks'] > 0 else 0
            theme_accuracy[theme].append(accuracy)
            
            # åˆ†ææ¯ä¸ªä»»åŠ¡çš„éš¾åº¦
            for task_key, task_result in result['task_results'].items():
                task_difficulty[task_key].append(1 if task_result['is_correct'] else 0)
        
        # è®¡ç®—ä¸»é¢˜çº§åˆ«çš„ç½®ä¿¡åº¦
        theme_confidence = {}
        for theme, accuracies in theme_accuracy.items():
            theme_confidence[theme] = {
                'mean_accuracy': np.mean(accuracies),
                'std_accuracy': np.std(accuracies),
                'sample_count': len(accuracies),
                'confidence_interval': np.percentile(accuracies, [25, 75]) if len(accuracies) > 1 else [0, 0]
            }
        
        # è®¡ç®—ä»»åŠ¡çº§åˆ«çš„éš¾åº¦
        task_confidence = {}
        task_names = {
            'atmosphere_recognition': 'æ°›å›´è¯†åˆ«',
            'ky_test': 'KYæµ‹è¯•',
            'collective_intent_inference': 'æ„å›¾æ¨æ–­'
        }
        
        for task_key, scores in task_difficulty.items():
            if scores:
                task_confidence[task_key] = {
                    'task_name': task_names.get(task_key, task_key),
                    'accuracy': np.mean(scores),
                    'total_samples': len(scores),
                    'correct_count': sum(scores),
                    'difficulty_level': self._classify_difficulty(np.mean(scores))
                }
        
        return {
            'theme_confidence': theme_confidence,
            'task_confidence': task_confidence
        }
    
    def _classify_difficulty(self, accuracy: float) -> str:
        """æ ¹æ®å‡†ç¡®ç‡åˆ†ç±»ä»»åŠ¡éš¾åº¦"""
        if accuracy >= 0.9:
            return "ç®€å•"
        elif accuracy >= 0.7:
            return "ä¸­ç­‰"
        elif accuracy >= 0.5:
            return "å›°éš¾"
        else:
            return "æå›°éš¾"
    
    def run_analysis(self) -> Dict:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ” å¼€å§‹åˆ†æAIä¸äººå·¥æ ‡æ³¨çš„ç½®ä¿¡åº¦å¯¹æ¯”...")
        
        # åŠ è½½æ•°æ®
        annotated_files = self.load_annotated_data()
        if not annotated_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ ‡æ³¨æ•°æ®æ–‡ä»¶")
            return self.results
        
        all_results = []
        total_samples = 0
        annotated_samples = 0
        
        # åˆ†ææ¯ä¸ªæ–‡ä»¶
        for file_info in annotated_files:
            data = file_info['data']
            samples = data.get('samples', [])
            total_samples += len(samples)
            
            for sample in samples:
                if sample.get('human_annotated', False):
                    result = self.analyze_sample_accuracy(sample)
                    if result:
                        all_results.append(result)
                        annotated_samples += 1
        
        if not all_results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°äººå·¥æ ‡æ³¨çš„æ ·æœ¬")
            return self.results
        
        print(f"ğŸ“Š åˆ†æå®Œæˆ: {annotated_samples}/{total_samples} ä¸ªæ ·æœ¬å·²æ ‡æ³¨")
        
        # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
        total_correct = sum(r['overall_correct'] for r in all_results)
        total_tasks = sum(r['total_tasks'] for r in all_results)
        overall_accuracy = total_correct / total_tasks if total_tasks > 0 else 0
        
        # è®¡ç®—å„ä»»åŠ¡å‡†ç¡®ç‡
        task_accuracy = {}
        task_names = {
            'atmosphere_recognition': 'æ°›å›´è¯†åˆ«',
            'ky_test': 'KYæµ‹è¯•',
            'collective_intent_inference': 'æ„å›¾æ¨æ–­'
        }
        
        for task_key, task_name in task_names.items():
            correct = sum(1 for r in all_results 
                         if task_key in r['task_results'] and r['task_results'][task_key]['is_correct'])
            total = sum(1 for r in all_results if task_key in r['task_results'])
            
            if total > 0:
                task_accuracy[task_key] = {
                    'task_name': task_name,
                    'accuracy': correct / total,
                    'correct_count': correct,
                    'total_count': total
                }
        
        # è®¡ç®—æ ‡æ³¨è€…é—´ä¸€è‡´æ€§
        agreement_matrix = self.calculate_inter_annotator_agreement(all_results)
        
        # ç”Ÿæˆç½®ä¿¡åº¦åˆ†æ
        confidence_analysis = self.generate_confidence_analysis(all_results)
        
        # æ±‡æ€»ç»“æœ
        self.results = {
            'total_samples': total_samples,
            'annotated_samples': annotated_samples,
            'overall_accuracy': overall_accuracy,
            'task_accuracy': task_accuracy,
            'agreement_matrix': agreement_matrix,
            'confidence_analysis': confidence_analysis,
            'detailed_results': all_results
        }
        
        return self.results
    
    def generate_report(self, output_file: str = "ai_human_comparison_report.json"):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        results = self.run_analysis()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ‘˜è¦æŠ¥å‘Š
        self.print_summary_report(results)
        
        return output_file
    
    def print_summary_report(self, results: Dict):
        """æ‰“å°æ‘˜è¦æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ¤– AIä¸äººå·¥æ ‡æ³¨ç½®ä¿¡åº¦å¯¹æ¯”åˆ†ææŠ¥å‘Š")
        print("="*80)
        
        print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"  æ€»æ ·æœ¬æ•°: {results['total_samples']}")
        print(f"  å·²æ ‡æ³¨æ ·æœ¬: {results['annotated_samples']}")
        print(f"  æ ‡æ³¨è¦†ç›–ç‡: {results['annotated_samples']/results['total_samples']*100:.1f}%")
        
        print(f"\nğŸ¯ æ•´ä½“å‡†ç¡®ç‡:")
        print(f"  DeepSeek V3 vs äººå·¥æ ‡æ³¨: {results['overall_accuracy']*100:.1f}%")
        
        print(f"\nğŸ“‹ å„ä»»åŠ¡å‡†ç¡®ç‡:")
        for task_key, task_data in results['task_accuracy'].items():
            print(f"  {task_data['task_name']}: {task_data['accuracy']*100:.1f}% "
                  f"({task_data['correct_count']}/{task_data['total_count']})")
        
        print(f"\nğŸ” ç½®ä¿¡åº¦åˆ†æ:")
        task_conf = results['confidence_analysis']['task_confidence']
        for task_key, conf_data in task_conf.items():
            print(f"  {conf_data['task_name']}: {conf_data['accuracy']*100:.1f}% "
                  f"(éš¾åº¦: {conf_data['difficulty_level']})")
        
        print(f"\nğŸ“ˆ ä¸»é¢˜å‡†ç¡®ç‡åˆ†å¸ƒ:")
        theme_conf = results['confidence_analysis']['theme_confidence']
        for theme, conf_data in theme_conf.items():
            print(f"  {theme}: {conf_data['mean_accuracy']*100:.1f}% Â± {conf_data['std_accuracy']*100:.1f}% "
                  f"(n={conf_data['sample_count']})")
        
        print(f"\nğŸ¤ æ ‡æ³¨ä¸€è‡´æ€§ (AI vs äººå·¥):")
        for task_key, agreement_data in results['agreement_matrix'].items():
            task_name = results['task_accuracy'][task_key]['task_name']
            print(f"  {task_name}: {agreement_data['agreement_rate']:.1f}% "
                  f"({agreement_data['agreements']}/{agreement_data['total_comparisons']})")
        
        print("\n" + "="*80)

def main():
    """ä¸»å‡½æ•°"""
    analyzer = AnnotationAnalyzer()
    report_file = analyzer.generate_report()
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")

if __name__ == "__main__":
    main()
