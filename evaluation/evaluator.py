"""
å¤šçº¿ç¨‹è¯„æµ‹å™¨ - æ”¯æŒå¹¶å‘è¯„æµ‹å’Œç»“æœåˆ†æ
"""
import json
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Any
import sys
from collections import defaultdict
import pandas as pd
import csv
import os
from datetime import datetime

# æ·»åŠ ä¸»ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from eval_client_bilingual import BilingualEvaluationClient

class MultiThreadEvaluator:
    """å¤šçº¿ç¨‹è¯„æµ‹å™¨"""
    
    def __init__(self, models: List[str] = None, max_workers: int = 4, use_siliconflow: bool = False, use_agentworld: bool = False, use_yunwu: bool = False, language: str = "zh", evaluation_mode: str = "full"):
        self.models = models or [
            "moonshotai/kimi-k2:free",
            "z-ai/glm-4.5-air:free"
        ]
        self.max_workers = max_workers
        self.results = {}
        self.use_siliconflow = use_siliconflow
        self.use_agentworld = use_agentworld
        self.use_yunwu = use_yunwu
        self.language = language
        self.evaluation_mode = evaluation_mode
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºç‹¬ç«‹çš„å®¢æˆ·ç«¯
        self.clients = {}
        for model in self.models:
            self.clients[model] = BilingualEvaluationClient([model], use_siliconflow=use_siliconflow, use_agentworld=use_agentworld, use_yunwu=use_yunwu, language=language, evaluation_mode=evaluation_mode)
        
        # CSVæ–‡ä»¶é”ï¼Œç¡®ä¿å¤šçº¿ç¨‹å†™å…¥å®‰å…¨
        self.csv_lock = threading.Lock()
        
        print(f"ğŸš€ å¤šçº¿ç¨‹è¯„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ¯ è¯„æµ‹æ¨¡å‹: {', '.join(self.models)}")
        print(f"ğŸ§µ æœ€å¤§çº¿ç¨‹æ•°: {max_workers}")
    
    def load_dataset(self, file_path: str) -> List[Dict]:
        """åŠ è½½æ•°æ®é›†"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            samples = data.get('samples', [])
            print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {file_path}")
            print(f"ğŸ“ æ ·æœ¬æ•°é‡: {len(samples)}")
            
            return samples
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            return []
    
    def evaluate_sample_task(self, sample: Dict, model: str, task_type: str) -> Dict:
        """è¯„æµ‹å•ä¸ªæ ·æœ¬çš„å•ä¸ªä»»åŠ¡"""
        try:
            client = self.clients[model]
            result = client.evaluate_sample(sample, task_type)
            
            if result:
                result.update({
                    'model': model,
                    'task_type': task_type,
                    'benchmark_id': sample['benchmark_id'],
                    'meta_theme': sample['meta_theme']
                })
            
            return result
            
        except Exception as e:
            print(f"âŒ è¯„æµ‹å¤±è´¥ {model} - {task_type}: {e}")
            return None
    
    def init_csv_file(self, output_path: Path) -> str:
        """åˆå§‹åŒ–CSVæ–‡ä»¶"""
        csv_file = output_path / "evaluation_results.csv"
        
        # åˆ›å»ºCSVæ–‡ä»¶å¤´
        fieldnames = [
            'timestamp', 'benchmark_id', 'meta_theme', 'model', 'task_type', 
            'predicted_answer', 'correct_answer', 'is_correct', 
            'raw_response', 'parse_error'
        ]
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
        
        print(f"ğŸ“„ CSVç»“æœæ–‡ä»¶åˆå§‹åŒ–: {csv_file}")
        return str(csv_file)
    
    def save_result_to_csv(self, result: Dict, csv_file: str):
        """ä¿å­˜å•ä¸ªç»“æœåˆ°CSVæ–‡ä»¶"""
        if not result or result.get('parse_error', False):
            return
        
        # å‡†å¤‡CSVè¡Œæ•°æ®
        row_data = {
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'benchmark_id': result.get('benchmark_id', ''),
            'meta_theme': result.get('meta_theme', ''),
            'model': result.get('model', ''),
            'task_type': result.get('task_type', ''),
            'predicted_answer': result.get('predicted_answer', -1),
            'correct_answer': result.get('correct_answer', -1),
            'is_correct': result.get('is_correct', False),
            'raw_response': result.get('raw_response', '').replace('\n', ' ').replace('\r', ' ')[:200],  # é™åˆ¶é•¿åº¦
            'parse_error': result.get('parse_error', False)
        }
        
        # çº¿ç¨‹å®‰å…¨åœ°å†™å…¥CSV
        with self.csv_lock:
            with open(csv_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=row_data.keys())
                writer.writerow(row_data)
    
    def evaluate_dataset(self, samples: List[Dict], output_dir: str = None) -> Dict:
        """è¯„æµ‹æ•´ä¸ªæ•°æ®é›†"""
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä½¿ç”¨æ—¶é—´æˆ³åˆ›å»º
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"results_{timestamp}"
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # å‡†å¤‡è¯„æµ‹ä»»åŠ¡
        tasks = []
        task_types = ["atmosphere_recognition", "ky_test", "subtext_deciphering"]
        
        for sample in samples:
            for model in self.models:
                for task_type in task_types:
                    tasks.append((sample, model, task_type))
        
        total_tasks = len(tasks)
        print(f"ğŸ¯ æ€»è¯„æµ‹ä»»åŠ¡æ•°: {total_tasks}")
        print(f"ğŸ“Š æ ·æœ¬æ•°: {len(samples)} | æ¨¡å‹æ•°: {len(self.models)} | ä»»åŠ¡ç±»å‹æ•°: {len(task_types)}")
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶
        csv_file = self.init_csv_file(output_path)
        
        # åˆå§‹åŒ–ç»“æœå­˜å‚¨
        results = {model: {task: [] for task in task_types} for model in self.models}
        completed_tasks = 0
        failed_tasks = 0
        successful_tasks = 0  # æ–°å¢ï¼šæˆåŠŸä»»åŠ¡è®¡æ•°
        
        start_time = time.time()
        
        # å¤šçº¿ç¨‹æ‰§è¡Œè¯„æµ‹
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(self.evaluate_sample_task, sample, model, task_type): (sample, model, task_type)
                for sample, model, task_type in tasks
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_task):
                sample, model, task_type = future_to_task[future]
                completed_tasks += 1
                
                try:
                    result = future.result()
                    if result and not result.get('parse_error', False):
                        # æˆåŠŸçš„ç»“æœ
                        results[model][task_type].append(result)
                        successful_tasks += 1
                        
                        # å®æ—¶ä¿å­˜åˆ°CSV
                        self.save_result_to_csv(result, csv_file)
                        
                        if successful_tasks % 10 == 0:
                            print(f"âœ… å·²æˆåŠŸè¯„æµ‹ {successful_tasks} ä¸ªä»»åŠ¡ï¼Œå®æ—¶ä¿å­˜åˆ°CSV")
                    else:
                        failed_tasks += 1
                        
                except Exception as e:
                    print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
                    failed_tasks += 1
                
                # è¿›åº¦æ˜¾ç¤º
                if completed_tasks % 20 == 0 or completed_tasks == total_tasks:
                    progress = completed_tasks / total_tasks * 100
                    elapsed = time.time() - start_time
                    eta = elapsed / completed_tasks * (total_tasks - completed_tasks) if completed_tasks > 0 else 0
                    
                    print(f"ğŸ“ˆ è¿›åº¦: {completed_tasks}/{total_tasks} ({progress:.1f}%) "
                          f"| æˆåŠŸ: {successful_tasks} | å¤±è´¥: {failed_tasks} "
                          f"| è€—æ—¶: {elapsed:.1f}s | é¢„è®¡å‰©ä½™: {eta:.1f}s")
        
        # ä¿å­˜åŸå§‹ç»“æœ
        raw_results_file = output_path / "raw_results.json"
        with open(raw_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ åŸå§‹ç»“æœå·²ä¿å­˜: {raw_results_file}")
        
        # åˆ†æç»“æœ (åªç»Ÿè®¡æˆåŠŸçš„æ ·æœ¬)
        analysis = self.analyze_results(results, samples, successful_tasks, failed_tasks)
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = output_path / "evaluation_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š åˆ†æç»“æœå·²ä¿å­˜: {analysis_file}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(analysis, output_path)
        
        return analysis
    
    def analyze_results(self, results: Dict, samples: List[Dict], successful_tasks: int, failed_tasks: int) -> Dict:
        """åˆ†æè¯„æµ‹ç»“æœ"""
        analysis = {
            'summary': {},
            'model_performance': {},
            'task_performance': {},
            'theme_performance': {},
            'detailed_comparison': {}
        }
        
        task_names = {
            'atmosphere_recognition': 'æ°›å›´è¯†åˆ«',
            'ky_test': 'KYæµ‹è¯•',
            'subtext_deciphering': 'æ½œå°è¯è§£ç '
        }
        
        # åˆ†ææ¯ä¸ªæ¨¡å‹çš„è¡¨ç°
        for model in self.models:
            model_stats = {
                'total_samples': 0,
                'correct_predictions': 0,
                'accuracy': 0.0,
                'task_accuracies': {}
            }
            
            for task_type, task_results in results[model].items():
                if not task_results:
                    continue
                
                correct = sum(1 for r in task_results if r.get('is_correct', False))
                total = len(task_results)
                accuracy = correct / total if total > 0 else 0
                
                model_stats['task_accuracies'][task_type] = {
                    'task_name': task_names.get(task_type, task_type),
                    'correct': correct,
                    'total': total,
                    'accuracy': accuracy
                }
                
                model_stats['total_samples'] += total
                model_stats['correct_predictions'] += correct
            
            if model_stats['total_samples'] > 0:
                model_stats['accuracy'] = model_stats['correct_predictions'] / model_stats['total_samples']
            
            analysis['model_performance'][model] = model_stats
        
        # åˆ†æä»»åŠ¡éš¾åº¦
        for task_type in task_names.keys():
            task_stats = {
                'task_name': task_names[task_type],
                'model_results': {},
                'average_accuracy': 0.0,
                'difficulty_level': ''
            }
            
            accuracies = []
            for model in self.models:
                if task_type in results[model] and results[model][task_type]:
                    task_results = results[model][task_type]
                    correct = sum(1 for r in task_results if r.get('is_correct', False))
                    total = len(task_results)
                    accuracy = correct / total if total > 0 else 0
                    
                    task_stats['model_results'][model] = {
                        'accuracy': accuracy,
                        'correct': correct,
                        'total': total
                    }
                    accuracies.append(accuracy)
            
            if accuracies:
                avg_acc = sum(accuracies) / len(accuracies)
                task_stats['average_accuracy'] = avg_acc
                
                # éš¾åº¦åˆ†çº§
                if avg_acc >= 0.8:
                    task_stats['difficulty_level'] = 'ç®€å•'
                elif avg_acc >= 0.6:
                    task_stats['difficulty_level'] = 'ä¸­ç­‰'
                elif avg_acc >= 0.4:
                    task_stats['difficulty_level'] = 'å›°éš¾'
                else:
                    task_stats['difficulty_level'] = 'æå›°éš¾'
            
            analysis['task_performance'][task_type] = task_stats
        
        # åˆ†æä¸»é¢˜è¡¨ç°
        theme_stats = defaultdict(lambda: defaultdict(list))
        
        for model in self.models:
            for task_type, task_results in results[model].items():
                for result in task_results:
                    theme = result.get('meta_theme', 'æœªçŸ¥')
                    is_correct = result.get('is_correct', False)
                    theme_stats[theme][model].append(is_correct)
        
        for theme, model_data in theme_stats.items():
            theme_analysis = {
                'theme_name': theme,
                'model_accuracies': {},
                'average_accuracy': 0.0
            }
            
            accuracies = []
            for model, correct_list in model_data.items():
                if correct_list:
                    accuracy = sum(correct_list) / len(correct_list)
                    theme_analysis['model_accuracies'][model] = {
                        'accuracy': accuracy,
                        'correct': sum(correct_list),
                        'total': len(correct_list)
                    }
                    accuracies.append(accuracy)
            
            if accuracies:
                theme_analysis['average_accuracy'] = sum(accuracies) / len(accuracies)
            
            analysis['theme_performance'][theme] = theme_analysis
        
        # è®¡ç®—æˆåŠŸè¯„æµ‹çš„æ ·æœ¬æ•° (åªç»Ÿè®¡æˆåŠŸçš„æ ·æœ¬)
        successfully_evaluated_samples = set()
        for model_results in results.values():
            for task_results in model_results.values():
                for result in task_results:
                    if result.get('is_correct') is not None:  # æœ‰æ•ˆç»“æœ
                        successfully_evaluated_samples.add(result.get('benchmark_id'))
        
        best_model = max(analysis['model_performance'].items(), 
                        key=lambda x: x[1]['accuracy']) if analysis['model_performance'] else None
        
        analysis['summary'] = {
            'total_samples_in_dataset': len(samples),
            'successfully_evaluated_samples': len(successfully_evaluated_samples),
            'successful_tasks': successful_tasks,
            'failed_tasks': failed_tasks,
            'success_rate': successful_tasks / (successful_tasks + failed_tasks) * 100 if (successful_tasks + failed_tasks) > 0 else 0,
            'models_tested': len(self.models),
            'tasks_tested': len(task_names),
            'best_model': best_model[0] if best_model else None,
            'best_model_accuracy': best_model[1]['accuracy'] if best_model else 0.0
        }
        
        return analysis
    
    def generate_report(self, analysis: Dict, output_path: Path):
        """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
        report_file = output_path / "evaluation_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# æ¨¡å‹è¯„æµ‹æŠ¥å‘Š\n\n")
            
            # æ¦‚è§ˆ
            summary = analysis['summary']
            f.write("## ğŸ“Š è¯„æµ‹æ¦‚è§ˆ\n\n")
            f.write(f"- **æ•°æ®é›†æ€»æ ·æœ¬æ•°**: {summary['total_samples_in_dataset']}\n")
            f.write(f"- **æˆåŠŸè¯„æµ‹æ ·æœ¬æ•°**: {summary['successfully_evaluated_samples']}\n")
            f.write(f"- **æˆåŠŸä»»åŠ¡æ•°**: {summary['successful_tasks']}\n")
            f.write(f"- **å¤±è´¥ä»»åŠ¡æ•°**: {summary['failed_tasks']}\n")
            f.write(f"- **ä»»åŠ¡æˆåŠŸç‡**: {summary['success_rate']:.1f}%\n")
            f.write(f"- **æµ‹è¯•æ¨¡å‹æ•°**: {summary['models_tested']}\n")
            f.write(f"- **è¯„æµ‹ä»»åŠ¡ç±»å‹æ•°**: {summary['tasks_tested']}\n")
            f.write(f"- **æœ€ä½³æ¨¡å‹**: {summary['best_model']} ({summary['best_model_accuracy']*100:.1f}%)\n\n")
            f.write("**æ³¨æ„**: ä»¥ä¸‹æ‰€æœ‰å‡†ç¡®ç‡ç»Ÿè®¡å‡åŸºäºæˆåŠŸè¯„æµ‹çš„æ ·æœ¬ï¼Œå¤±è´¥çš„è¯„æµ‹ä»»åŠ¡ä¸è®¡å…¥ç»Ÿè®¡ã€‚\n\n")
            
            # æ¨¡å‹è¡¨ç°
            f.write("## ğŸ¤– æ¨¡å‹è¡¨ç°\n\n")
            f.write("| æ¨¡å‹ | æ€»ä½“å‡†ç¡®ç‡ | æ°›å›´è¯†åˆ« | KYæµ‹è¯• | æ½œå°è¯è§£ç  |\n")
            f.write("|------|------------|----------|--------|----------|\n")
            
            for model, stats in analysis['model_performance'].items():
                f.write(f"| {model} | {stats['accuracy']*100:.1f}% |")
                
                for task_type in ['atmosphere_recognition', 'ky_test', 'subtext_deciphering']:
                    if task_type in stats['task_accuracies']:
                        acc = stats['task_accuracies'][task_type]['accuracy']
                        f.write(f" {acc*100:.1f}% |")
                    else:
                        f.write(" N/A |")
                f.write("\n")
            
            f.write("\n")
            
            # ä»»åŠ¡éš¾åº¦åˆ†æ
            f.write("## ğŸ“‹ ä»»åŠ¡éš¾åº¦åˆ†æ\n\n")
            for task_type, task_stats in analysis['task_performance'].items():
                f.write(f"### {task_stats['task_name']}\n")
                f.write(f"- **å¹³å‡å‡†ç¡®ç‡**: {task_stats['average_accuracy']*100:.1f}%\n")
                f.write(f"- **éš¾åº¦ç­‰çº§**: {task_stats['difficulty_level']}\n")
                
                f.write("- **å„æ¨¡å‹è¡¨ç°**:\n")
                for model, result in task_stats['model_results'].items():
                    f.write(f"  - {model}: {result['accuracy']*100:.1f}% ({result['correct']}/{result['total']})\n")
                f.write("\n")
            
            # ä¸»é¢˜è¡¨ç°åˆ†æ
            f.write("## ğŸ­ ä¸»é¢˜è¡¨ç°åˆ†æ\n\n")
            for theme, theme_stats in analysis['theme_performance'].items():
                f.write(f"### {theme}\n")
                f.write(f"- **å¹³å‡å‡†ç¡®ç‡**: {theme_stats['average_accuracy']*100:.1f}%\n")
                
                f.write("- **å„æ¨¡å‹è¡¨ç°**:\n")
                for model, result in theme_stats['model_accuracies'].items():
                    f.write(f"  - {model}: {result['accuracy']*100:.1f}% ({result['correct']}/{result['total']})\n")
                f.write("\n")
        
        print(f"ğŸ“„ è¯„æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    def print_summary(self, analysis: Dict):
        """æ‰“å°è¯„æµ‹æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ¯ æ¨¡å‹è¯„æµ‹æ‘˜è¦")
        print("="*80)
        
        summary = analysis['summary']
        print(f"ğŸ“Š æ•°æ®é›†æ€»æ ·æœ¬æ•°: {summary['total_samples_in_dataset']}")
        print(f"âœ… æˆåŠŸè¯„æµ‹æ ·æœ¬æ•°: {summary['successfully_evaluated_samples']}")
        print(f"ğŸ“ˆ ä»»åŠ¡æˆåŠŸç‡: {summary['success_rate']:.1f}% ({summary['successful_tasks']}/{summary['successful_tasks'] + summary['failed_tasks']})")
        print(f"ğŸ¤– æœ€ä½³æ¨¡å‹: {summary['best_model']} ({summary['best_model_accuracy']*100:.1f}%)")
        print(f"âš ï¸  æ³¨æ„: å‡†ç¡®ç‡ç»Ÿè®¡ä»…åŸºäºæˆåŠŸè¯„æµ‹çš„æ ·æœ¬")
        
        print(f"\nğŸ“‹ æ¨¡å‹æ’å:")
        sorted_models = sorted(
            analysis['model_performance'].items(),
            key=lambda x: x[1]['accuracy'],
            reverse=True
        )
        
        for i, (model, stats) in enumerate(sorted_models, 1):
            print(f"  {i}. {model}: {stats['accuracy']*100:.1f}%")
        
        print(f"\nğŸ¯ ä»»åŠ¡éš¾åº¦:")
        for task_type, task_stats in analysis['task_performance'].items():
            print(f"  {task_stats['task_name']}: {task_stats['average_accuracy']*100:.1f}% ({task_stats['difficulty_level']})")
        
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤šçº¿ç¨‹æ¨¡å‹è¯„æµ‹å™¨")
    parser.add_argument("--data", required=True, help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--models", nargs="+", 
                       default=["moonshotai/kimi-k2:free", "z-ai/glm-4.5-air:free"],
                       help="è¦è¯„æµ‹çš„æ¨¡å‹åˆ—è¡¨")
    parser.add_argument("--workers", type=int, default=4, help="æœ€å¤§çº¿ç¨‹æ•°")
    parser.add_argument("--output", default=None, help="ç»“æœè¾“å‡ºç›®å½•(é»˜è®¤ä½¿ç”¨æ—¶é—´æˆ³)")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„æµ‹å™¨
    evaluator = MultiThreadEvaluator(models=args.models, max_workers=args.workers)
    
    # åŠ è½½æ•°æ®é›†
    samples = evaluator.load_dataset(args.data)
    if not samples:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®é›†")
        return
    
    # æ‰§è¡Œè¯„æµ‹
    print(f"\nğŸš€ å¼€å§‹è¯„æµ‹...")
    analysis = evaluator.evaluate_dataset(samples, args.output)
    
    # æ‰“å°æ‘˜è¦
    evaluator.print_summary(analysis)
    
    # æ‰“å°å®¢æˆ·ç«¯ç»Ÿè®¡
    for model, client in evaluator.clients.items():
        print(f"\n{model} å®¢æˆ·ç«¯ç»Ÿè®¡:")
        client.print_stats()

if __name__ == "__main__":
    main()
